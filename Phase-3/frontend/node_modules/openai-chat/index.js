import fetch from 'node-fetch'
const DEFAULT = {model: 0, name: 'ai', context: 2, prompt: ''}

export class AIChannel{
	static models = ['gpt-3.5-turbo-instruct']
	static pricing = [0.0000015, 0.000002] // Dollars per token
	static usage = 0
	static marvPreset = {name: 'marv', model: 0, context: 1, prompt: 'marv likes banter and can be quite rude'}
	static blobPreset = {name: 'blob', model: 0, context: 2, prompt: 'blob is a really friendly guy and really understands people.'}
	/**
	 * Make a new AI context!
	 * @param {string} token 
	 * @param {{model: number, name: string, context: number, prompt: string}} [options={model: 2, name: 'bot', context: 2, prompt: 'conversation online'}] **model**: Currently has no effect. **context**: How much of the conversation he remembers. Beware! this could radically increase costs. __**name**__: custom name for your bot, may change how bot percieves self and what kind of replies you get. Experiment with different names! __**prompt**__: describe how the bot should respond. Works best with a name. E.g 'marv is a really rude bot'. Warning: keep prompt short as it counts towards usage for every request!
	 * @example const channel = new AIChannel('sk-...', {name: 'dave', prompt: 'dave is very friendly and polite'}); console.log(await channel.get('Hello!', 'john')) //e.g "Hello! How are you?"
	 * @example const channel = new AIChannel('sk-...', AIChannel.marvPreset); console.log(await channel.get('Hello!', 'john')) //e.g "Get lost idiot"
	 */
	constructor(token, options = DEFAULT){
		for(const i in DEFAULT)if(!(i in options))options[i] = DEFAULT[i]
		const model = AIChannel.models[0] //[+options.model]
		if(!model)throw new RangeError("option 'model' must be 0")
		this.name = (options.name+'').replace(/\W/g, '')
		this.token = token
		this.history = []
		this.maxhistory = (+options.context + 1 || 3) - 1
		this.model = model
		this.pricingIn = AIChannel.pricing[0] //[options.model<<1]
		this.pricingOut = AIChannel.pricing[1] //[options.model<<1|1]
		this.usage = 0
		this.quotaHit = null; this.error = null
		this.prompt = options.prompt||''
	}
	/**
	 * Get a natural response to a prompt (message)
	 * @param {string} message message to respond to
	 * @param {string} user name of user (recommended but not required)
	 * @param {number} [max_tokens=64] Maximum tokens this request is allowed to use
	 * @returns {Promise<string>} response, or an empty string if an error occured
	 */
	async get(message, user = 'user', max_tokens = 64, retry_if_fail = true){
		user = (''+user).replace(/\W/g, '') //clean input
		const prompt = user+': '+message+'\n'+this.name+':'
		try{
			const {error, choices, usage} = await fetch('https://api.openai.com/v1/completions', {
			method: 'POST',
			headers: {"Content-Type":"application/json",Authorization:"Bearer "+this.token},
			body:'{"model":"'+this.model+'","prompt":'+JSON.stringify(this.prompt+'\n'+this.history.join('')+prompt)+',"temperature":1,"max_tokens":'+(+max_tokens||64)+',"top_p":1,"frequency_penalty":0.5,"presence_penalty":1.5'+(retry_if_fail?',"stop":["\\n"]':'')+'}'}).then(a=>a.json())
			if(error){
				if(error.type == 'insufficient_quota' && this.quotaHit) this.quotaHit()
				if(this.error) this.error(error)
				return ''
			}
			const cost = (usage.completion_tokens??0) * this.pricingOut + (usage.prompt_tokens??0) * this.pricingIn
			AIChannel.usage += cost
			this.usage += cost
			const {0:txt} = (choices.find(a => a.text && !a.text.startsWith('\n')) || {text: ''}).text.trim().split("\n", 1)
			if(!txt && retry_if_fail)return await this.get(message, user, max_tokens, false)
			if(this.history.length>=this.maxhistory){
				const l = this.history.length - 1
				for(let i=0;i<l;i++)this.history[i] = this.history[i + 1]
				this.history[l] = prompt+' '+txt+'\n'
			}else this.history.push(prompt+' '+txt+'\n')
			return txt
		}catch(e){return ''}
	}
	/**
	 * Clear context (start a new, fresh conversation)
	 */
	clearContext(){this.history.length = 0}
	/**
	 * Generate an image from a prompt! Warning: VERY Expensive
	 * @param {string} prompt 
	 * @param {number} [size=3] 1, 2 or 3 
	 * @returns {Promise<string>} the URL of the image (hosted for you)
	 */
	static imageSizes = ['256x256', '256x256', '512x512', '1024x1024']
	static imagePrices = [0.016, 0.018, 0.02]
	async dalle(prompt, size = 2){
		const {error, data: [{url}] = [0]} = await fetch('https://api.openai.com/v1/images/generations', {method:'POST',headers: {'Content-Type': 'application/json', Authorization: 'Bearer '+this.token}, body: '{"prompt":'+JSON.stringify(''+prompt)+',"n":1,"size":"'+AIChannel.imageSizes[size&3]+'"}'}).then(a=>a.json())
		if(error){
			if(error.type == 'insufficient_quota'){
				if(this.quotaHit) this.quotaHit()
			}
			if(this.error) this.error(error)
			return ''
		}
		this.usage += AIChannel.imagePrices[size&3]
		return url
	}
}

export class ChatChannel{
	static pricing = 0.000002 // Dollars per token
	static usage = 0
	static marvPreset = {name: 'marv', context: 2, prompt: 'You are Marv. Marv likes banter and can be quite rude'}
	static blobPreset = {name: 'blob', context: 2, prompt: 'You are Blob. Blob is a really friendly guy and really understands people.'}
	/**
	 * Make a new AI context!
	 * ChatChannel is an UPGRADED version of AIChannel which uses the much more powerful model, gpt-3.5-turbo-0301
	 * @param {string} token 
	 * @param {{name: string, context: number, prompt: string}} [options={model: 2, name: 'bot', context: 2, prompt: 'conversation online'}] **context**: How much of the conversation he remembers. Beware! this could radically increase costs. __**name**__: custom name for your bot, may change how bot percieves self and what kind of replies you get. Experiment with different names! __**prompt**__: describe how the bot should respond. Works best with a name. E.g 'marv is a really rude bot'. Warning: keep prompt short as it counts towards usage for every request!
	 * @example const channel = new ChatChannel('sk-...', {name: 'dave', prompt: 'dave is very friendly and polite'}); console.log(await channel.get('Hello!', 'john')) //e.g "Hello! How are you?"
	 * @example const channel = new ChatChannel('sk-...', ChatChannel.marvPreset); console.log(await channel.get('Hello!', 'john')) //e.g "Get lost idiot"
	 */
	constructor(token, options = DEFAULT){
		for(const i in DEFAULT)if(!(i in options))options[i] = DEFAULT[i]
		this.name = (options.name+'').replace(/\W/g, '')
		this.token = token
		this.history = []
		this.maxhistory = (+options.context + 1 || 3) - 1
		this.usage = 0
		this.prompt = options.prompt||''
		this.quotaHit = null; this.error = null
		if(this.prompt) this.system(this.prompt)
	}
	/**
	 * Get a natural response to a prompt (message)
	 * Note that this model tends to give much more polite and politically correct responses.
	 * @param {string} message message to respond to
	 * @param {string} user name of user (recommended but not required)
	 * @param {number} [max_tokens=64] Maximum tokens this request is allowed to use
	 * @returns {Promise<string>} response, or an empty string if an error occured
	 */
	system(msg){
		msg = 'system","content":'+JSON.stringify(msg)
		if(this.history.length>=this.maxhistory){
			const l = this.history.length - 1
			for(let i=+!!this.prompt;i<l;i++)this.history[i] = this.history[i + 1]
			this.history[l] = msg
		}else this.history.push(msg)
	}
	async get(message, user = 'user', max_tokens = 64, retry_if_fail = true){
		user = (''+user).replace(/\W/g, '').toLowerCase() //clean input
		message = message.trim().replace(/\.?\n/g, '.  ')
		if(!message) return ''
		const prompt = 'user","content":'+JSON.stringify(user+": "+message)
		this.history.push(prompt)
		try{
			const {error, choices: m, usage} = await (await fetch('https://api.openai.com/v1/chat/completions', {
				method: 'POST', headers: { "Content-Type":"application/json",Authorization:"Bearer "+this.token },
				body: '{"model":"gpt-3.5-turbo-1106","messages":[{"role":"'+this.history.join('},{"role":"')+'}],"temperature":1,"max_tokens":'+(+max_tokens||64)+',"top_p":1,"frequency_penalty":0.5,"presence_penalty":1.5}'
			})).json()
			if(error){
				if(error.type == 'insufficient_quota'){
					if(this.quotaHit) this.quotaHit()
				}
				if(this.error) this.error(error)
				return ''
			}
			if((!m || !m.length) && retry_if_fail)return await this.get(message, user, max_tokens, false)
			ChatChannel.usage += usage.total_tokens * ChatChannel.pricing
			this.usage += usage.total_tokens * ChatChannel.pricing
			let res = m[0].message.content
			const role = res.match(/^[\w\s]+:\s*/)
			if(role) res = res.slice(role[0].length)
			if(this.history.length>=this.maxhistory){
				const l = this.history.length - 1
				for(let i=+!!this.prompt;i<l;i++)this.history[i] = this.history[i + 1]
				this.history[l] = 'assistant","content":'+JSON.stringify(this.name+": "+res)
			}else this.history.push('assistant","content":'+JSON.stringify(this.name+": "+res))
			return res
		}catch(e){return ''}
	}
	/**
	 * Clear context (start a new, fresh conversation)
	 */
	clearContext(){this.history.length = +!!this.prompt}
	/**
	 * Generate an image from a prompt! Warning: VERY Expensive
	 * @param {string} prompt 
	 * @param {number} [size=3] 1, 2 or 3 
	 * @returns {Promise<string>} the URL of the image (hosted for you)
	 */
	static imageSizes = ['256x256', '256x256', '512x512', '1024x1024']
	static imagePrices = [0.016, 0.018, 0.02]
	async dalle(prompt, size = 2){
		const {error, data: [{url}] = [0]} = await fetch('https://api.openai.com/v1/images/generations', {method:'POST',headers: {'Content-Type': 'application/json', Authorization: 'Bearer '+this.token}, body: '{"prompt":'+JSON.stringify(''+prompt)+',"n":1,"size":"'+AIChannel.imageSizes[size&3]+'"}'}).then(a=>a.json())
		if(error){
			if(error.type == 'insufficient_quota'){
				if(this.quotaHit) this.quotaHit()
			}
			if(this.error) this.error(error)
			return ''
		}
		this.usage += AIChannel.imagePrices[size&3]
		return url
	}
}

export default ChatChannel